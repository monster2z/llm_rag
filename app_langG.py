import streamlit as st
import streamlit_authenticator as stauth
import yaml
from yaml.loader import SafeLoader
import time
import os
import tempfile
import pandas as pd
from typing import Annotated, Sequence, TypedDict, Union, List, Dict, Any
import operator
import json
from enum import Enum
import uuid

# LangChain ì„í¬íŠ¸ (ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œë¡œ ë³€ê²½)
from langchain_community.document_loaders import (
    PyPDFLoader, 
    Docx2txtLoader, 
    CSVLoader, 
    UnstructuredPowerPointLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS  # Chroma ëŒ€ì‹  FAISS ì‚¬ìš©
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# LangGraph ì„í¬íŠ¸
from langgraph.graph import StateGraph, END
from langgraph.pregel import Pregel

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# langsmithë¡œ ë¡œê¹… ì„¤ì •
try:
    from langchain_teddynote import logging
    logging.langsmith("llm_rag_prototype")
except ImportError:
    print("langchain_teddynote ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œê¹… ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

# SQLite ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ pysqlite3 ì„¤ì •
try:
    import pysqlite3
    import sys
    sys.modules['sqlite3'] = pysqlite3
except ImportError:
    print("pysqlite3 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. sqlite3 ê´€ë ¨ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
api_key = os.environ.get("OPENAI_API_KEY")
admin_pass = os.environ.get("ADMIN_PASS")
user_pass = os.environ.get("USER_PASS")
# anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY")  # í•„ìš”ì‹œ í™œì„±í™”

# ì„ë² ë”© ëª¨ë¸ ì„ íƒ
EMBEDDING_MODEL = "text-embedding-3-small"

# LLM ëª¨ë¸ ì„ íƒ
LLM_MODEL = "gpt-4o-mini"  # ë˜ëŠ” "claude-3-sonnet-20240229"
LLM_PROVIDER = "openai"  # ë˜ëŠ” "anthropic"

# LangGraph ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    question: str
    context: List[str] 
    answer: str
    conversation_history: List[Dict[str, str]]
    sources: List[Dict[str, str]]
    need_more_info: bool
    username: str  # ì‚¬ìš©ì ì‹ë³„ì„ ìœ„í•œ í•„ë“œ ì¶”ê°€

# ì„ì‹œ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
DATA_DIR = "./data"
os.makedirs(DATA_DIR, exist_ok=True)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ê¸°ì—… ë‚´ë¶€ìš© LLM í”„ë¡œí† íƒ€ì…", layout="wide")

# ì‘ë‹µ ìƒì„± í•¨ìˆ˜ - í•¨ìˆ˜ ìœ„ì¹˜ ì´ë™
def generate_response(prompt, username, messages_key):
    # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    conversation_history = [
        {"role": msg["role"], "content": msg["content"]} 
        for msg in st.session_state[messages_key]
    ]
    
    # LangGraph ì›Œí¬í”Œë¡œìš°ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
    if 'rag_workflow' in st.session_state and 'vectorstore' in st.session_state:
        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {
                "question": prompt,
                "context": [],
                "answer": "",
                "conversation_history": conversation_history,
                "sources": [],
                "need_more_info": False,
                "username": username
            }
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            result = st.session_state.rag_workflow.invoke(initial_state)
            
            # ë‹µë³€ ë°˜í™˜
            return result["answer"]
            
        except Exception as e:
            st.error(f"RAG ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ í´ë°±
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    else:
        # ê¸°ë³¸ LLM ì‚¬ìš© (RAGê°€ ì—†ëŠ” ê²½ìš°)
        llm = ChatOpenAI(model=LLM_MODEL, api_key=api_key)
            
        template = """
        ë‹¹ì‹ ì€ ê¸°ì—… ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        
        í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
        ë‹¤ë§Œ, ì‚¬ìš©ìì—ê²Œ ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ì¢‹ì„ ê²ƒì´ë¼ê³  ì•Œë ¤ì£¼ì„¸ìš”.
        
        ì´ì „ ëŒ€í™” ê¸°ë¡: {conversation_history}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        prompt_template = ChatPromptTemplate.from_template(template)
        chain = prompt_template | llm | StrOutputParser()
        
        try:
            return chain.invoke({
                "question": prompt,
                "conversation_history": str(conversation_history)
            })
        except Exception as e:
            st.error(f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤

# ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
def retrieve_documents(state: AgentState) -> AgentState:
    """ë¬¸ì„œ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    # ì„¸ì…˜ ìƒíƒœì—ì„œ ë²¡í„° ì €ì¥ì†Œ ê°€ì ¸ì˜¤ê¸°
    vectorstore = st.session_state.get("vectorstore")
    
    if not vectorstore:
        return {**state, "context": [], "sources": []}
    
    # ê²€ìƒ‰ ìˆ˜í–‰
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    docs = retriever.get_relevant_documents(state["question"])
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    contexts = []
    sources = []
    
    for doc in docs:
        contexts.append(doc.page_content)
        sources.append({
            "source": doc.metadata.get("source_file", "Unknown"),
            "page": doc.metadata.get("page", "N/A")
        })
    
    return {**state, "context": contexts, "sources": sources}

# ì§ˆë¬¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_answer(state: AgentState) -> AgentState:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    if LLM_PROVIDER == "anthropic":
        llm = ChatAnthropic(model=LLM_MODEL)
    else:
        llm = ChatOpenAI(model=LLM_MODEL, api_key=api_key)
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    template = """
    ë‹¹ì‹ ì€ ê¸°ì—… ë‚´ë¶€ ë¬¸ì„œì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ë¬¸ë§¥ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    ë¬¸ë§¥ ì •ë³´ì— ë‹µì´ ì—†ëŠ” ê²½ìš°, "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ê³  
    need_more_infoë¥¼ Trueë¡œ ì„¤ì •í•˜ì„¸ìš”. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Falseë¡œ ì„¤ì •í•˜ì„¸ìš”.
    
    ì´ì „ ëŒ€í™” ê¸°ë¡: {conversation_history}
    
    ë¬¸ë§¥ ì •ë³´:
    {context}
    
    ì§ˆë¬¸: {question}
    
    ë‹µë³€:
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = "\n\n".join(state["context"]) if state["context"] else "ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ì´ì „ ëŒ€í™” ê¸°ë¡
    conversation_history = state.get("conversation_history", [])
    
    # ì…ë ¥ êµ¬ì„±
    inputs = {
        "question": state["question"],
        "context": context_text,
        "conversation_history": str(conversation_history)
    }
    
    # ë‹µë³€ ìƒì„±
    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke(inputs)
    
    # ì¶”ê°€ ì •ë³´ í•„ìš” ì—¬ë¶€ íŒë‹¨
    need_more_info = "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in answer
    
    return {
        **state, 
        "answer": answer, 
        "need_more_info": need_more_info
    }

# ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€ í•¨ìˆ˜
def add_source_information(state: AgentState) -> AgentState:
    """ë‹µë³€ì— ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    if not state["sources"]:
        return state
        
    sources_info = "\n\n**ì°¸ê³  ë¬¸ì„œ:**\n"
    for src in state["sources"]:
        sources_info += f"- {src['source']}"
        if src['page'] != "N/A":
            sources_info += f" (í˜ì´ì§€: {src['page']})"
        sources_info += "\n"
    
    enhanced_answer = state["answer"] + sources_info
    
    return {**state, "answer": enhanced_answer}

# LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± í•¨ìˆ˜
def create_rag_workflow():
    """RAG ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    # ì›Œí¬í”Œë¡œìš° ì •ì˜
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("generate", generate_answer)
    workflow.add_node("add_sources", add_source_information)
    
    # ì—£ì§€ ì„¤ì •
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "add_sources")
    workflow.add_edge("add_sources", END)
    
    # ì‹œì‘ ë…¸ë“œ ì„¤ì •
    workflow.set_entry_point("retrieve")
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    return workflow.compile()

# ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© í•¨ìˆ˜
def process_documents(uploaded_files):
    documents = []
    file_info = []
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200
    )
    
    for uploaded_file in uploaded_files:
        # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
        file_type = uploaded_file.name.split('.')[-1].lower()
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_type}') as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_file_path = temp_file.name
            
        try:
            # ë¡œë” ì„ íƒ ë° ë¬¸ì„œ ë¡œë“œ
            loader = get_loader(temp_file_path, file_type)
            loaded_documents = loader.load()
            
            # ë¬¸ì„œ ë¶„í• 
            split_documents = text_splitter.split_documents(loaded_documents)
            
            # íŒŒì¼ ì •ë³´ ì¶”ê°€ (ë©”íƒ€ë°ì´í„°)
            for doc in split_documents:
                if not doc.metadata:
                    doc.metadata = {}
                doc.metadata["source_file"] = uploaded_file.name
                doc.metadata["file_type"] = file_type
                # ì—…ë¡œë” ì •ë³´ ì¶”ê°€ (ì‚¬ìš©ìë³„ ë¬¸ì„œ ê´€ë¦¬ë¥¼ ìœ„í•´)
                doc.metadata["uploaded_by"] = st.session_state["username"]
            
            documents.extend(split_documents)
            file_info.append({
                "filename": uploaded_file.name,
                "file_type": file_type,
                "chunks": len(split_documents),
                "uploaded_by": st.session_state["username"],
                "upload_time": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            st.sidebar.success(f"{uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ - {len(split_documents)}ê°œ ì²­í¬ ìƒì„±")
        except Exception as e:
            st.sidebar.error(f"{uploaded_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_file_path)
    
    # ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    if documents:
        st.sidebar.info("ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=api_key)
        
        # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ì¶”ê°€, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if "vectorstore" in st.session_state:
            try:
                # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€
                st.session_state.vectorstore.add_documents(documents)
                vectorstore = st.session_state.vectorstore
            except Exception as e:
                st.sidebar.warning(f"ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {str(e)}")
                # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                vectorstore = FAISS.from_documents(documents, embeddings)
        else:
            # ìƒˆ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = FAISS.from_documents(documents, embeddings)
        
        # ë¡œì»¬ì— ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (ë‚˜ì¤‘ì— ë¡œë“œí•  ìˆ˜ ìˆë„ë¡)
        vector_store_path = os.path.join(DATA_DIR, f"faiss_index_{uuid.uuid4().hex}")
        os.makedirs(vector_store_path, exist_ok=True)
        vectorstore.save_local(vector_store_path)
        
        st.sidebar.success(f"ì„ë² ë”© ì™„ë£Œ! {len(documents)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ë¨")
        
        # íŒŒì¼ ì •ë³´ ì €ì¥
        if "uploaded_files_info" not in st.session_state:
            st.session_state.uploaded_files_info = []
        st.session_state.uploaded_files_info.extend(file_info)
        
        return vectorstore, file_info
    return None, []

# íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ë¡œë” ì„ íƒ í•¨ìˆ˜
def get_loader(file_path, file_type):
    if file_type == 'pdf':
        return PyPDFLoader(file_path)
    elif file_type == 'docx':
        return Docx2txtLoader(file_path)
    elif file_type == 'csv':
        return CSVLoader(file_path)
    elif file_type == 'pptx':
        return UnstructuredPowerPointLoader(file_path)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}")

# ì‚¬ìš©ìë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
def get_user_conversations(username):
    """ì‚¬ìš©ìë³„ ëŒ€í™” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    user_conv_key = f"conversations_{username}"
    if user_conv_key not in st.session_state:
        st.session_state[user_conv_key] = ["ìƒˆ ëŒ€í™”"]
    return st.session_state[user_conv_key]

def add_conversation(username, title=None):
    """ì‚¬ìš©ì ëŒ€í™” ì¶”ê°€"""
    user_conv_key = f"conversations_{username}"
    if title is None:
        title = f"ëŒ€í™” {len(st.session_state[user_conv_key])}"
    st.session_state[user_conv_key].append(title)
    return len(st.session_state[user_conv_key]) - 1  # ìƒˆ ëŒ€í™” ì¸ë±ìŠ¤ ë°˜í™˜

def get_conversation_messages(username, conv_index):
    """íŠ¹ì • ëŒ€í™”ì˜ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°"""
    messages_key = f"messages_{username}_{conv_index}"
    if messages_key not in st.session_state:
        st.session_state[messages_key] = []
    return messages_key

# ì‚¬ìš©ì ì¸ì¦ ì„¤ì • íŒŒì¼ ìƒì„± í•¨ìˆ˜
def create_config_file():
    hasher = stauth.Hasher()
    
    config = {
        'credentials': {
            'usernames': {
                'admin1': {
                    'email': 'admin@example.com',
                    'name': 'ê´€ë¦¬ì',
                    'password': hasher.hash(admin_pass)
                },
                'user17': {
                    'email': 'user1@example.com',
                    'name': 'ì‚¬ìš©ì1',
                    'password': hasher.hash(user_pass)
                }
            }
        },
        'cookie': {
            'expiry_days': 30,
            'key': 'some_signature_key',
            'name': 'llm_dashboard_cookie'
        }
    }
    
    with open('config.yaml', 'w', encoding='utf-8') as file:
        yaml.dump(config, file, default_flow_style=False, allow_unicode=True)
    
    return config

# ì„¤ì • íŒŒì¼ ë¡œë“œ ë˜ëŠ” ìƒì„±
if not os.path.exists('config.yaml'):
    config = create_config_file()
else:
    with open('config.yaml', 'r', encoding='utf-8') as file:
        config = yaml.load(file, Loader=SafeLoader)

# ì¸ì¦ ê°ì²´ ìƒì„±
authenticator = stauth.Authenticate(
    config['credentials'],
    config['cookie']['name'],
    config['cookie']['key'],
    config['cookie']['expiry_days']
)

# ë¡œê·¸ì¸ í™”ë©´ - 0.4.x ë²„ì „ì— ë§ì¶¤
authenticator.login()

# ë¡œê·¸ì¸ ìƒíƒœì— ë”°ë¥¸ í™”ë©´ ì „í™˜
if st.session_state["authentication_status"]:
    # ë¡œê·¸ì¸ ì„±ê³µ í›„ ë©”ì¸ í™”ë©´
    authenticator.logout('ë¡œê·¸ì•„ì›ƒ', 'sidebar')
    
    # í˜„ì¬ ì‚¬ìš©ì ì •ë³´ ì €ì¥
    if "username" not in st.session_state:
        st.session_state["username"] = st.session_state["username"]  # ì‚¬ìš©ì ID
        
    username = st.session_state["username"]  
    st.sidebar.title(f'{st.session_state["name"]} ë‹˜ í™˜ì˜í•©ë‹ˆë‹¤')
    
    # ì‚¬ì´ë“œë°” - ëŒ€í™” ëª©ë¡ ì˜ì—­
    with st.sidebar:
        st.title("ëŒ€í™” ëª©ë¡")
        
        # ì‚¬ìš©ìë³„ ëŒ€í™” ëª©ë¡ ê´€ë¦¬
        conversations = get_user_conversations(username)
        
        # í˜„ì¬ ëŒ€í™” ìƒíƒœ ê´€ë¦¬
        if f"current_conversation_{username}" not in st.session_state:
            st.session_state[f"current_conversation_{username}"] = 0
            
        current_conv_idx = st.session_state[f"current_conversation_{username}"]
        
        # ëŒ€í™” ëª©ë¡ í‘œì‹œ
        for i, conv in enumerate(conversations):
            if st.button(f"{conv}", key=f"conv_{i}"):
                st.session_state[f"current_conversation_{username}"] = i
                st.rerun()
        
        # ìƒˆ ëŒ€í™” ë²„íŠ¼
        if st.button("ìƒˆ ëŒ€í™” ì‹œì‘"):
            # ìƒˆ ëŒ€í™” ì¶”ê°€
            new_idx = add_conversation(username)
            st.session_state[f"current_conversation_{username}"] = new_idx
            st.rerun()
            
        # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
        st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader("ê¸°ì—… ë‚´ë¶€ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", 
                                       type=['pdf', 'docx', 'csv', 'pptx'], 
                                       accept_multiple_files=True)
        
        # íŒŒì¼ ì²˜ë¦¬ ë²„íŠ¼
        if uploaded_files and st.button("ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”©"):
            with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
                vectorstore, file_info = process_documents(uploaded_files)
                if vectorstore:
                    st.session_state.vectorstore = vectorstore
                    # LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
                    st.session_state.rag_workflow = create_rag_workflow()
                    st.success("ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    # ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
                    if file_info:
                        st.subheader("ì²˜ë¦¬ëœ íŒŒì¼")
                        for file in file_info:
                            st.write(f"ğŸ“„ {file['filename']} - {file['chunks']}ê°œ ì²­í¬")
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
        if "uploaded_files_info" in st.session_state and st.session_state.uploaded_files_info:
            st.header("ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡")
            # ì‚¬ìš©ìë³„ íŒŒì¼ í•„í„°ë§
            user_files = [f for f in st.session_state.uploaded_files_info if f.get("uploaded_by") == username]
            
            if user_files:
                for file in user_files:
                    st.write(f"ğŸ“„ {file['filename']} - {file['upload_time']}")
            else:
                st.write("ì—…ë¡œë“œí•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ë©”ì¸ ì»¨í…Œì´ë„ˆ - ì±„íŒ… ì˜ì—­
    st.title("ê¸°ì—… ë‚´ë¶€ìš© AI ì–´ì‹œìŠ¤í„´íŠ¸")
    
    # í˜„ì¬ ëŒ€í™” ì¸ë±ìŠ¤
    current_conv_idx = st.session_state[f"current_conversation_{username}"]
    
    # í˜„ì¬ ëŒ€í™”ì˜ ë©”ì‹œì§€ í‚¤
    current_messages_key = get_conversation_messages(username, current_conv_idx)
    
    # ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state[current_messages_key]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
    
    # ì…ë ¥ ì²˜ë¦¬
    if prompt:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state[current_messages_key].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                response = generate_response(prompt, username, current_messages_key)
            message_placeholder.markdown(response)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
        st.session_state[current_messages_key].append({"role": "assistant", "content": response})

elif st.session_state["authentication_status"] == False:
    st.error('ì‚¬ìš©ìëª…/ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤')
elif st.session_state["authentication_status"] is None:
    st.warning('ì‚¬ìš©ìëª…ê³¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”')